# IJCAI2020GMR

## abstract:

图记忆重构：
    节点：状态（的聚类中心）
    边：已经尝试过的动作
    边的权重：（频次_无奖励/动作的值函数_有奖励）
    图中的链： 策略
    链上的权： 策略的值函数
    记忆编码是：节点的构造，相似节点的聚类，
    记忆的存储：节点的加入，边的加入，边权的改变
    （核心）记忆的重构：节点状态的更新，信息的传播，重新聚类

核心内容是通过GNN 的信息传播来完成策略的更新。

两种流程：

main1: 在记忆中训练好策略（策略收敛），在实际中执行
    
main2: 在实际任务中训练策略，训练过程中将记忆检索结果作为奖励值的一部分。

## introduction

main1:

1. 初始化智能体：
    所在环境--已知动作，得到状态（先初始化一个确定环境，不确定环境中要引入概率图计算）
    随机策略--智能体能根据随机种子产生随机数
    空的记忆--初始化图的建立和修正函数


2.随机采样：
    输入随机策略
    与环境交互 
    得到观测与转移--能完整的执行n个episode
    状态编码（聚类）--不同的状态编码不同，相似的状态编码欧式距离接近
    动作链接 -- 能把状态转移体现到图中
    权重赋值（有奖励用值函数，没有用访问频次的倒数）-- 根据每个episode 的结果进行讨论

3.记忆推理：
    当前图结构作为输入
    记忆检索： z_cur 当前状态编码能在图中找到对应节点z_start
    记忆生成： 由一个状态出发，rollout 不同的链式结构
    策略训练： 用推理数据训练智能体策略网络（策略网络如果不是时序相关的，那么直接对数据进行采样不也一样吗？要么就直接是没有策略网络这种东西了，更新策略就是更新图权重）

4.实际执行：
    执行当前图的策略+探索因子
    （可以解决起始点不同的问题，不同的z_start到图中检索）
    得到环境的数据

5.记忆重构：
    环境数据编码（不同的场景要有不同的编码器，这一点暂时还做不到）
    图结构进行补充，得到新的图
    对所有图的边权进行更新（可以尝试剪枝-遗忘，以减轻记忆压力）
    得到新的记忆图结构

6.循环第3步


main2:

1.初始化智能体：
    环境：先用静态环境，动作空间较小的，链接不会太多
    策略：
    记忆：

2.随机采样：
    随机策略
    得到环境的初步感知
    状态编码：
    动作链接：
    权重赋值：

3.记忆推理：
    记忆检索：一个输入状态z_cur, 查寻相似状态
    记忆生成：rollout多个连接上的轨迹
    记忆重构：得到各个动作的值

4.实际执行：
    根据记忆值和当前观测，执行动作
    3，
    得到环境反馈
    5

5.记忆重构：
    根据一次数据（或者一回合数据）
    进行图聚类
    根据s,r,s_,a 修正图链接

6.循环第三步

## Related work

main2 基本模块及参考：
1.要有一个简单的环境：静态的，确定环境，动作空间不能太大，奖励要有多个，前后最好能有相似的结构可以提供给智能体进行参考

参考deepmind 的pycolab
tvt中包含的是一个三段式结构，第二段是纯干扰项
尝试pycolab中的其它选项 cite from https://github.com/deepmind/pycolab
 python pycolab/examples/better_scrolly_maze.py 是个迷宫环境，有多处相似的迷宫结构，且不是静态环境

在conda activate DeepmindResearch中运行示例：python pycolab/examples/scrolly_maze.py 

使用时需要封装，参考tvt中的做法,在pycolab中用env.py进行包装，目标是得到，action_space,obs_space ,step, reward, render 等函数. tvt 代码中用active visual match 重新包装了一次，这里不进行包装。只是简单转化。
没调试成功？？？
s
先用maze进行后续流程

2.要有一个从记忆中获得内部奖励的简单模块：
用它来完成基本的执行流程
    def get_action_value(self, state):
        '''
        根据图中节点找到可执行的边的权重??????


3.要有一个能够构建图结构的模块：

sorb中是先构建状态对儿，再构建图
直接用s,s_构造图



4.图聚类和重构的模块:
小图可以不聚类，只是简单重构即可

## methods

1. 先把实验环境选定

2. 构造随机智能体

3. 随机探索，形成基本的记忆编码，图构建，生成轨迹

4. 引入内部奖励机制修饰探索过程

5. 当前状态在图中的检索

目前是直接使用

6. 图中的rollout 


7. 根据rollout定义内部奖励值
目前是根据奖励值定义的，接下来要根据值函数定义，或者用图进行传播
先用值函数，计算一条轨迹结束后(是否可以每一步改动一次？？？)，各个状态上的值，然后成比例累加到边上

对比qlearning 中，实际上是只更新边的权重得到的结果

节点的attribute 怎么和边的attribute进行 信息传递

8. 训练策略

9.效果评估 
在基本可以完成之后尝试评估效果
python mainGq.py
首先和  Q使用同样的参数，看看能否得到相似的效果
回合更新，单次更新

从实验效果上看还是有很大差距的，找到问题的源头，保证是从一个正确的起点出发

然后对每个改进版本进行评估

几个问题：
为什么要聚类节点，什么时候聚类，按照什么标准聚类，怎么聚类，聚类的结果如何使用

1.为啥要聚类

自动得到状态宏
可以得到更宏观的抽象
加速探索过程，避免在同一个团簇内反复探索

2.什么时候聚类
每个回合结束后

3.按照什么标准聚类
边上的权重（q值）用来完成强化学习过程，因为它与奖励值相关
边上的其他信息，如访问次数，step 值，用来辅助完成节点的聚类，（聚类的主要元素要是节点的特征）

4.怎么聚类
canopy


5.聚类的结果如何使用
类似于分层
类内鼓励利用，类间鼓励探索
在类内平均各个边的值，让他们处于同等地位


实施步骤：
构造简单可视化数据，
但是数据结构要和真实一样，可以少跑几次然后放出来
实现聚类和中心节点的提取
对聚类后的边做近似提取，也就是将值在区域内进行传播，（可以是节点值，也可以是边的值）