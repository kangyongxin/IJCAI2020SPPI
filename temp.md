临时文件，整理之前的代码，找到主要问题以及相应的解决方案,整理之后重新建立文件夹，准备新的论文

目前的做法：
行动过程中不进行推演，按图执行，并加入一定的随机探索。在一个回合之后，回头重新构造自己的记忆，将构造的结果通过图的形式存下来。


图的基本构成：
节点（node）：状态（或者是聚类的中心）  
边(edge)：（已经尝试过的）动作
节点的特征(attribute)：状态的特征向量（也可以有诸如值函数v的特征，或者访问次数，当然奖励也是必不可少的节点特征）
边的特征（weight）:最简单的是有无和方向，还要有相应的状态动作值函数，（也可以有访问次数）
链（sequence）：策略（策略组合）
图（graph）：记忆
图的更新（update of graph）：记忆的重构

用了图之后我们可以做很多之前不太方便做的事情：比如记忆重构，比如节点聚类，比如导航任务

## introduction
在记忆中训练好策略（策略收敛），在实际中执行
1.初始化智能体：
    所在环境--已知动作，得到状态（先初始化一个确定环境，不确定环境中要引入概率图计算）
    随机策略--智能体能根据随机种子产生随机数
    空的记忆--初始化图的建立和修正函数

2.随机采样：
    输入随机策略
    与环境交互 
    得到观测与转移--能完整的执行n个episode
    状态编码（聚类）--不同的状态编码不同，相似的状态编码欧式距离接近
    动作链接 -- 能把状态转移体现到图中
    权重赋值（有奖励用值函数，没有用访问频次的倒数）-- 根据每个episode 的结果进行讨论

3.记忆推理：
    当前图结构作为输入
    记忆检索： z_cur 当前状态编码能在图中找到对应节点z_start
    记忆生成： 由一个状态出发，rollout 不同的链式结构（这一步目前还没有引入，我们只是rollout一步也就是相当于one-step Q）
    策略训练： 用推理数据训练智能体策略网络（策略网络如果不是时序相关的，那么直接对数据进行采样不也一样吗？要么就直接是没有策略网络这种东西了，更新策略就是更新图权重）   

4.实际执行：
    执行当前图的策略+探索因子
    （可以解决起始点不同的问题，不同的z_start到图中检索）
    得到环境的数据 

5.记忆重构：
    环境数据编码（不同的场景要有不同的编码器，这一点暂时还做不到）
    进行图聚类（目前来看并不是必须的）
    图结构进行补充，得到新的图
    对所有图的边权进行更新（可以尝试剪枝-遗忘，以减轻记忆压力）
    得到新的记忆图结构

6.循环第3步

参考代码:
world model 采用Atari作为基本环境
tvt 中可能有我们需要的环境，以及相应的编码，存储，检索模块
sorb 中有相应的构建图的模块，现在已经用上了
我们是不是要把二者的环境完全跑通做为实验对比呢

