# IJCAI2020GMR

## abstract:

图记忆重构：
    节点：状态（的聚类中心）
    边：已经尝试过的动作
    边的权重：（频次_无奖励/动作的值函数_有奖励）
    图中的链： 策略
    链上的权： 策略的值函数
    记忆编码是：节点的构造，相似节点的聚类，
    记忆的存储：节点的加入，边的加入，边权的改变
    （核心）记忆的重构：节点状态的更新，信息的传播，重新聚类

核心内容是通过GNN 的信息传播来完成策略的更新。

两种流程：

main1: 在记忆中训练好策略（策略收敛），在实际中执行
    
main2: 在实际任务中训练策略，训练过程中将记忆检索结果作为奖励值的一部分。

## introduction

main1:

1. 初始化智能体：
    所在环境--已知动作，得到状态（先初始化一个确定环境，不确定环境中要引入概率图计算）
    随机策略--智能体能根据随机种子产生随机数
    空的记忆--初始化图的建立和修正函数


2.随机采样：
    输入随机策略
    与环境交互 
    得到观测与转移--能完整的执行n个episode
    状态编码（聚类）--不同的状态编码不同，相似的状态编码欧式距离接近
    动作链接 -- 能把状态转移体现到图中
    权重赋值（有奖励用值函数，没有用访问频次的倒数）-- 根据每个episode 的结果进行讨论

3.记忆推理：
    当前图结构作为输入
    记忆检索： z_cur 当前状态编码能在图中找到对应节点z_start
    记忆生成： 由一个状态出发，rollout 不同的链式结构
    策略训练： 用推理数据训练智能体策略网络（策略网络如果不是时序相关的，那么直接对数据进行采样不也一样吗？要么就直接是没有策略网络这种东西了，更新策略就是更新图权重）

4.实际执行：
    执行当前图的策略+探索因子
    （可以解决起始点不同的问题，不同的z_start到图中检索）
    得到环境的数据

5.记忆重构：
    环境数据编码（不同的场景要有不同的编码器，这一点暂时还做不到）
    图结构进行补充，得到新的图
    对所有图的边权进行更新（可以尝试剪枝-遗忘，以减轻记忆压力）
    得到新的记忆图结构

6.循环第3步


main2:

1.初始化智能体：
    环境：先用静态环境，动作空间较小的，链接不会太多
    策略：
    记忆：

2.随机采样：
    随机策略
    得到环境的初步感知
    状态编码：
    动作链接：
    权重赋值：

3.记忆推理：
    记忆检索：一个输入状态z_cur, 查寻相似状态
    记忆生成：rollout多个连接上的轨迹
    记忆重构：得到各个动作的值

4.实际执行：
    根据记忆值和当前观测，执行动作
    3，
    得到环境反馈
    5

5.记忆重构：
    根据一次数据（或者一回合数据）
    进行图聚类
    根据s,r,s_,a 修正图链接

6.循环第三步

## Related work

main2 基本模块及参考：
1.要有一个简单的环境：静态的，确定环境，动作空间不能太大，奖励要有多个，前后最好能有相似的结构可以提供给智能体进行参考

参考deepmind 的pycolab
tvt中包含的是一个三段式结构，第二段是纯干扰项
尝试pycolab中的其它选项 cite from https://github.com/deepmind/pycolab
 python pycolab/examples/better_scrolly_maze.py 是个迷宫环境，有多处相似的迷宫结构，且不是静态环境

在conda activate DeepmindResearch中运行示例：python pycolab/examples/scrolly_maze.py 

使用时需要封装，参考tvt中的做法,在pycolab中用env.py进行包装，目标是得到，action_space,obs_space ,step, reward, render 等函数. tvt 代码中用active visual match 重新包装了一次，这里不进行包装。只是简单转化。
没调试成功？？？
s
先用maze进行后续流程

2.要有一个从记忆中获得内部奖励的简单模块：
用它来完成基本的执行流程
    def get_action_value(self, state):
        '''
        根据图中节点找到可执行的边的权重??????


3.要有一个能够构建图结构的模块：

sorb中是先构建状态对儿，再构建图
直接用s,s_构造图



4.图聚类和重构的模块:
小图可以不聚类，只是简单重构即可

## methods

1. 先把实验环境选定

2. 构造随机智能体

3. 随机探索，形成基本的记忆编码，图构建，生成轨迹

4. 引入内部奖励机制修饰探索过程

5. 当前状态在图中的检索

6. 图中的rollout 

7. 根据rollout定义内部奖励值
目前是根据奖励值定义的，接下来要根据值函数定义，或者用图进行传播

8. 训练策略

9.效果评估 


